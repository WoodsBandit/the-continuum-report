# SOP-003: Brief Generation

**Document Version:** 1.0
**Last Updated:** 2025-12-25
**Pipeline Stage:** 3 of 4
**Trigger:** Change to connection_contexts.json
**Execution Mode:** Autonomous (Claude Code)

---

## 1. Purpose

Generate and update analytical intelligence products (entity briefs and connection briefs) based on accumulated source material and relationship contexts. This stage transforms raw data into publishable analytical content while ensuring legal compliance through automated review.

## 2. Trigger Condition

**Primary Trigger:** Modification timestamp change on `connection_contexts.json`
**Monitoring Method:** File system watch or periodic polling (every 30 seconds)
**Trigger Script:**
```python
# Watch connection_contexts.json for changes
import os
import time

contexts_path = r"\\192.168.1.139\continuum\indexes\connection_contexts.json"
last_modified = os.path.getmtime(contexts_path)

while True:
    current_modified = os.path.getmtime(contexts_path)
    if current_modified > last_modified:
        # File changed - trigger Stage 3
        execute_stage3()
        last_modified = current_modified
    time.sleep(30)
```

**Fallback Trigger:** Manual execution via command line with entity/connection filter

## 3. Prerequisites

### Required Files Must Exist
- `\\192.168.1.139\continuum\indexes\connection_contexts.json` (recently updated by Stage 2)
- `\\192.168.1.139\continuum\indexes\co_occurrence.json`
- `\\192.168.1.139\continuum\indexes\entity_registry.json`
- `\\192.168.1.139\continuum\indexes\source_mentions.json`
- `\\192.168.1.139\continuum\templates\entity-brief-template.md`
- `\\192.168.1.139\continuum\templates\connection-brief-template.md`

### Required Directories Must Exist
- `\\192.168.1.139\continuum\briefs\entity\`
- `\\192.168.1.139\continuum\briefs\connections\`
- `\\192.168.1.139\continuum\pending_approval\entities\`
- `\\192.168.1.139\continuum\pending_approval\connections\`

### System Requirements
- Legal-auditor agent accessible via Claude Code
- Write access to briefs/ and pending_approval/ directories
- Sufficient disk space for generated briefs

## 4. Inputs

### Primary Input
**Connection Contexts:** `\\192.168.1.139\continuum\indexes\connection_contexts.json`
- Focus on connections with `last_updated` == today (newly updated in Stage 2)

### Reference Inputs

**Entity Registry:** `\\192.168.1.139\continuum\indexes\entity_registry.json`
**Co-Occurrence Index:** `\\192.168.1.139\continuum\indexes\co_occurrence.json`
**Source Mentions:** `\\192.168.1.139\continuum\indexes\source_mentions.json`

### Template Inputs

**Entity Brief Template:** `\\192.168.1.139\continuum\templates\entity-brief-template.md`
```markdown
---
entity_name: "{ENTITY_NAME}"
entity_type: "{ENTITY_TYPE}"
brief_version: "{VERSION}"
last_updated: "{TIMESTAMP}"
sources_count: {COUNT}
legal_review: "PENDING"
---

# Analytical Brief: {ENTITY_NAME}

## Executive Summary
{AUTO_GENERATED_SUMMARY}

## Entity Profile
**Type:** {ENTITY_TYPE}
**First Documented:** {FIRST_SEEN_DATE}
**Last Mentioned:** {LAST_SEEN_DATE}
**Total Source Mentions:** {MENTION_COUNT}

## Source Analysis

### Primary Sources
{LIST_OF_KEY_SOURCES}

### Timeline of Mentions
{CHRONOLOGICAL_TIMELINE}

## Key Associations
{TOP_CONNECTIONS_WITH_CONTEXT}

## Notable Quotes & Statements
{EXTRACTED_QUOTES}

## Activity Summary
{ANALYSIS_OF_ENTITY_ACTIVITY}

## Source References
{FORMATTED_SOURCE_LIST}

---
*Generated by The Continuum Report Intelligence Pipeline*
*Last Updated: {TIMESTAMP}*
```

**Connection Brief Template:** `\\192.168.1.139\continuum\templates\connection-brief-template.md`

**ARCHITECTURAL PRINCIPLE:** Connection briefs are the source of truth. No connection exists without a brief. Each brief contains: quote + source + summary.

```markdown
---
connection_id: "{ENTITY1}|{ENTITY2}"
entity1: "{ENTITY1}"
entity2: "{ENTITY2}"
brief_version: "{VERSION}"
last_updated: "{TIMESTAMP}"
sources_count: {COUNT}
legal_review: "PENDING"
---

# Connection Brief: {ENTITY1} ↔ {ENTITY2}

## Executive Summary
{AUTO_GENERATED_SUMMARY}

## Connection Overview
**Sources Count:** {COUNT}
**First Documented:** {FIRST_DATE}
**Last Documented:** {LAST_DATE}
**Relationship Type:** {CONTEXT_TYPE}

## Evidence of Connection

### Source Evidence
{CHRONOLOGICAL_CONTEXT_SNIPPETS}

### Interaction Timeline
{TIMELINE_OF_INTERACTIONS}

## Relationship Analysis
{SYNTHESIZED_ANALYSIS}

## Additional Context
**Other Entities in Context:** {RELATED_ENTITIES}
**Common Locations:** {LOCATIONS}
**Time Period:** {DATE_RANGE}

## Source References
{FORMATTED_SOURCE_LIST}

---
*Generated by The Continuum Report Intelligence Pipeline*
*Last Updated: {TIMESTAMP}*
```

## 5. Process Steps

### Step 1: Identify Affected Entities and Connections

**Action:** Determine which briefs need creation or updating

```
READ: \\192.168.1.139\continuum\indexes\connection_contexts.json
READ: \\192.168.1.139\continuum\indexes\entity_registry.json

# Get last processing timestamp
READ: \\192.168.1.139\continuum\logs\stage3_last_run.txt
last_run_timestamp = PARSE timestamp from file

# Identify changed connections
changed_connections = []
FOR EACH connection IN connection_contexts.connections:
    IF connection.last_updated > last_run_timestamp:
        changed_connections.append(connection_id)
        LOG INFO: "Connection {connection_id} has new context"

# Identify affected entities (entities in changed connections)
affected_entities = set()
FOR EACH connection_id IN changed_connections:
    connection = connection_contexts.connections[connection_id]
    affected_entities.add(connection.entity1)
    affected_entities.add(connection.entity2)

# Also check for entities with new sources (not just new connections)
FOR EACH entity_name IN entity_registry.entities:
    entity = entity_registry.entities[entity_name]
    IF entity.last_seen > last_run_timestamp:
        affected_entities.add(entity_name)

LOG INFO: "Identified {len(affected_entities)} affected entities"
LOG INFO: "Identified {len(changed_connections)} changed connections"

IF len(affected_entities) == 0 AND len(changed_connections) == 0:
    LOG INFO: "No briefs to generate or update, Stage 3 complete"
    HALT processing (success)
```

### Step 2: Process Entity Briefs (CREATE or UPDATE)

**Action:** For each affected entity, determine if brief exists and whether to create or update

```
entity_briefs_to_generate = []

FOR EACH entity_name IN affected_entities:
    brief_path = \\192.168.1.139\continuum\briefs\entity\analytical_brief_{entity_name}.md

    # Normalize filename (replace special chars)
    brief_filename = NORMALIZE_FILENAME(f"analytical_brief_{entity_name}.md")
    brief_path = \\192.168.1.139\continuum\briefs\entity\{brief_filename}

    IF FILE_EXISTS(brief_path):
        # EXISTING BRIEF - Check if update needed
        LOG INFO: "Existing brief found for {entity_name}"

        # Read existing brief
        existing_brief = READ file at brief_path

        # Parse frontmatter to get current sources
        existing_metadata = PARSE_YAML_FRONTMATTER(existing_brief)
        existing_sources = existing_metadata.get("sources_covered", [])

        # Get current sources for entity
        entity = entity_registry.entities[entity_name]
        current_sources = entity.sources

        # Check for new sources
        new_sources = [s for s in current_sources if s not in existing_sources]

        IF len(new_sources) > 0:
            LOG INFO: "Entity {entity_name} has {len(new_sources)} new sources"

            # Assess if new sources contain significant information
            significant_new_info = ASSESS_NEW_INFORMATION(
                entity_name=entity_name,
                new_sources=new_sources,
                existing_brief=existing_brief
            )

            IF significant_new_info:
                entity_briefs_to_generate.append({
                    "entity_name": entity_name,
                    "action": "UPDATE",
                    "existing_brief": existing_brief,
                    "existing_metadata": existing_metadata,
                    "new_sources": new_sources,
                    "reason": f"New source material: {', '.join(new_sources)}"
                })
                LOG INFO: "Queued UPDATE for {entity_name}"
            ELSE:
                LOG INFO: "New sources for {entity_name} contain no significant new information"
        ELSE:
            LOG INFO: "No new sources for {entity_name}, checking connection updates"

            # Check if entity appears in new connections (relationship changes)
            entity_connections = GET_CONNECTIONS_FOR_ENTITY(entity_name)
            new_connections = [c for c in entity_connections if c in changed_connections]

            IF len(new_connections) > 0:
                # Entity has new relationships - may warrant brief update
                entity_briefs_to_generate.append({
                    "entity_name": entity_name,
                    "action": "UPDATE",
                    "existing_brief": existing_brief,
                    "existing_metadata": existing_metadata,
                    "new_sources": [],
                    "new_connections": new_connections,
                    "reason": f"New connections: {len(new_connections)}"
                })
                LOG INFO: "Queued UPDATE for {entity_name} (new connections)"

    ELSE:
        # NEW BRIEF - Create from all sources
        LOG INFO: "No existing brief for {entity_name}, creating new brief"

        entity_briefs_to_generate.append({
            "entity_name": entity_name,
            "action": "CREATE",
            "existing_brief": None,
            "existing_metadata": None,
            "new_sources": None,  # Use all sources
            "reason": "New entity discovered"
        })
        LOG INFO: "Queued CREATE for {entity_name}"

LOG INFO: "Prepared {len(entity_briefs_to_generate)} entity briefs for generation"
```

**Assess New Information Function:**
```
FUNCTION ASSESS_NEW_INFORMATION(entity_name, new_sources, existing_brief):
    # Read new source content
    new_content = []
    FOR EACH source_id IN new_sources:
        source_data = source_mentions.sources[source_id]
        source_path = source_data.file_path

        TRY:
            source_text = READ file at source_path
            # Extract mentions of entity
            mentions = EXTRACT_ENTITY_MENTIONS(entity_name, source_text)
            new_content.extend(mentions)

        EXCEPT FileNotFoundError:
            LOG WARNING: "Source file not found: {source_path}"
            CONTINUE

    # Use Claude to assess significance
    prompt = f'''
Compare the existing analytical brief with new source material for entity "{entity_name}".

Existing Brief:
---
{existing_brief}
---

New Source Material:
---
{new_content}
---

Determine if the new source material contains SIGNIFICANT new information that warrants updating the brief.

Significant new information includes:
- New quotes or statements by/about the entity
- New connections or associations
- New timeline events or activities
- Changed status, role, or position
- Contradictory information requiring analysis
- Legal developments

NOT significant:
- Redundant mentions of already-covered information
- Minor variations of existing quotes
- Same activities in different sources

Return JSON:
{{
  "is_significant": true/false,
  "reason": "Brief explanation",
  "new_elements": ["list", "of", "new", "elements"]
}}
'''

    result = CALL_CLAUDE(prompt)
    RETURN result.is_significant
```

### Step 3: Generate Entity Briefs

**Action:** Create or update entity analytical briefs

```
FOR EACH brief_spec IN entity_briefs_to_generate:
    entity_name = brief_spec.entity_name
    action = brief_spec.action  # "CREATE" or "UPDATE"

    LOG INFO: "Generating entity brief: {entity_name} ({action})"

    # Gather all data for entity
    entity = entity_registry.entities[entity_name]

    # Get all sources
    IF action == "CREATE":
        sources_to_analyze = entity.sources  # All sources
    ELSE:
        sources_to_analyze = brief_spec.new_sources  # Only new sources

    # Collect source content
    source_contents = []
    FOR EACH source_id IN sources_to_analyze:
        source_data = source_mentions.sources[source_id]
        source_path = source_data.file_path

        TRY:
            source_text = READ file at source_path
            # Extract context around entity mentions
            contexts = EXTRACT_ENTITY_CONTEXTS(entity_name, source_text, window=500)

            source_contents.append({
                "source_id": source_id,
                "title": source_data.title,
                "date": source_data.date,
                "contexts": contexts
            })

        EXCEPT FileNotFoundError:
            LOG WARNING: "Source file not found: {source_path}"
            CONTINUE

    # Get entity connections
    entity_connections = GET_CONNECTIONS_FOR_ENTITY(entity_name)
    connection_contexts = []
    FOR EACH connection_id IN entity_connections:
        IF connection_id IN connection_contexts.connections:
            conn_data = connection_contexts.connections[connection_id]
            connection_contexts.append({
                "connected_entity": conn_data.entity1 if conn_data.entity2 == entity_name else conn_data.entity2,
                "relationship_type": DETERMINE_PRIMARY_CONTEXT_TYPE(conn_data.contexts),
                "sources": [c.source_id for c in conn_data.contexts]
            })

    # Generate brief using template and Claude
    IF action == "CREATE":
        brief_content = GENERATE_NEW_ENTITY_BRIEF(
            entity_name=entity_name,
            entity_data=entity,
            source_contents=source_contents,
            connections=connection_contexts,
            template_path="\\192.168.1.139\continuum\templates\entity-brief-template.md"
        )

    ELSE:  # UPDATE
        brief_content = UPDATE_ENTITY_BRIEF(
            entity_name=entity_name,
            entity_data=entity,
            existing_brief=brief_spec.existing_brief,
            existing_metadata=brief_spec.existing_metadata,
            new_source_contents=source_contents,
            new_connections=connection_contexts
        )

    # Write brief to briefs/ directory
    brief_filename = NORMALIZE_FILENAME(f"analytical_brief_{entity_name}.md")
    brief_output_path = \\192.168.1.139\continuum\briefs\entity\{brief_filename}

    WRITE brief_content to brief_output_path

    LOG INFO: "Entity brief saved: {brief_output_path}"

    # Store for legal review
    entity_briefs_generated.append({
        "entity_name": entity_name,
        "brief_path": brief_output_path,
        "brief_content": brief_content,
        "action": action
    })
```

**Generate New Entity Brief Function:**
```
FUNCTION GENERATE_NEW_ENTITY_BRIEF(entity_name, entity_data, source_contents, connections, template_path):

    READ template from template_path

    # Use Claude to generate comprehensive brief
    prompt = f'''
Generate a comprehensive analytical intelligence brief for entity: "{entity_name}"

Entity Data:
- Type: {entity_data.entity_type}
- First Seen: {entity_data.first_seen}
- Last Seen: {entity_data.last_seen}
- Total Mentions: {entity_data.mention_count}
- Aliases: {entity_data.aliases}

Source Material:
{FORMAT_SOURCE_CONTENTS(source_contents)}

Connections:
{FORMAT_CONNECTIONS(connections)}

Template Structure:
{template}

Instructions:
1. Write an executive summary (2-3 paragraphs) synthesizing key information
2. Create a chronological timeline of entity mentions across sources
3. Highlight key associations with other entities, including context
4. Extract notable quotes or statements by/about the entity
5. Provide activity summary analyzing entity's role and significance
6. List all source references with titles and dates
7. Use neutral, factual tone - present evidence without speculation
8. Flag any contradictions or inconsistencies in source material
9. Do NOT make claims not supported by source material

Return complete markdown brief following template structure.
'''

    brief_content = CALL_CLAUDE(prompt)

    # Parse and add metadata
    metadata = {
        "entity_name": entity_name,
        "entity_type": entity_data.entity_type,
        "brief_version": "1.0",
        "last_updated": current_timestamp,
        "sources_count": len(entity_data.sources),
        "sources_covered": entity_data.sources,
        "legal_review": "PENDING"
    }

    # Combine metadata and content
    final_brief = FORMAT_BRIEF_WITH_METADATA(metadata, brief_content)

    RETURN final_brief
```

**Update Entity Brief Function:**
```
FUNCTION UPDATE_ENTITY_BRIEF(entity_name, entity_data, existing_brief, existing_metadata, new_source_contents, new_connections):

    # Use Claude to intelligently update existing brief
    prompt = f'''
Update an existing analytical intelligence brief for entity: "{entity_name}"

Existing Brief:
---
{existing_brief}
---

New Source Material:
{FORMAT_SOURCE_CONTENTS(new_source_contents)}

New Connections:
{FORMAT_CONNECTIONS(new_connections)}

Instructions:
1. Integrate new information into existing brief sections
2. Update Executive Summary if new information changes overall assessment
3. Add new timeline entries chronologically
4. Add new connections to Key Associations section
5. Add new quotes to Notable Quotes section
6. Update Activity Summary with new developments
7. Add new sources to Source References
8. Preserve existing analysis unless contradicted by new sources
9. If contradictions exist, note them explicitly
10. Maintain consistent tone and structure

Return complete updated markdown brief.
'''

    updated_brief_content = CALL_CLAUDE(prompt)

    # Update metadata
    updated_metadata = existing_metadata.copy()
    updated_metadata["brief_version"] = INCREMENT_VERSION(existing_metadata.brief_version)
    updated_metadata["last_updated"] = current_timestamp
    updated_metadata["sources_count"] = len(entity_data.sources)
    updated_metadata["sources_covered"] = entity_data.sources

    # Combine metadata and content
    final_brief = FORMAT_BRIEF_WITH_METADATA(updated_metadata, updated_brief_content)

    RETURN final_brief
```

### Step 4: Process Connection Briefs (CREATE or UPDATE)

**Action:** For each changed connection, determine if brief exists and whether to create or update

```
connection_briefs_to_generate = []

FOR EACH connection_id IN changed_connections:
    connection_data = connection_contexts.connections[connection_id]

    # Check co-mention threshold
    co_occurrence_data = co_occurrence.pairs[connection_id]
    co_mention_count = co_occurrence_data.co_mention_count

    IF co_mention_count < 2:
        LOG INFO: "Connection {connection_id} has insufficient co-mentions ({co_mention_count}), skipping brief"
        CONTINUE

    # Check for substantive context
    high_relevance_contexts = [c for c in connection_data.contexts if c.relevance_score >= 0.5]

    IF len(high_relevance_contexts) == 0:
        LOG INFO: "Connection {connection_id} has no high-relevance contexts, skipping brief"
        CONTINUE

    # Generate filename
    brief_filename = NORMALIZE_FILENAME(f"{connection_data.entity1}_{connection_data.entity2}.md")
    brief_path = \\192.168.1.139\continuum\briefs\connections\{brief_filename}

    IF FILE_EXISTS(brief_path):
        # EXISTING BRIEF - Update with new context
        LOG INFO: "Existing connection brief found for {connection_id}"

        existing_brief = READ file at brief_path
        existing_metadata = PARSE_YAML_FRONTMATTER(existing_brief)

        # Get existing context source IDs
        existing_context_sources = existing_metadata.get("context_sources", [])

        # Check for new contexts
        new_contexts = [c for c in connection_data.contexts if c.source_id not in existing_context_sources]

        IF len(new_contexts) > 0:
            connection_briefs_to_generate.append({
                "connection_id": connection_id,
                "action": "UPDATE",
                "existing_brief": existing_brief,
                "existing_metadata": existing_metadata,
                "new_contexts": new_contexts,
                "reason": f"New context from {len(new_contexts)} sources"
            })
            LOG INFO: "Queued UPDATE for connection {connection_id}"
        ELSE:
            LOG INFO: "No new contexts for connection {connection_id}"

    ELSE:
        # NEW BRIEF - Create from all contexts
        LOG INFO: "No existing brief for connection {connection_id}, creating new brief"

        connection_briefs_to_generate.append({
            "connection_id": connection_id,
            "action": "CREATE",
            "existing_brief": None,
            "existing_metadata": None,
            "new_contexts": None,  # Use all contexts
            "reason": "New connection discovered"
        })
        LOG INFO: "Queued CREATE for connection {connection_id}"

LOG INFO: "Prepared {len(connection_briefs_to_generate)} connection briefs for generation"
```

### Step 5: Generate Connection Briefs

**Action:** Create or update connection briefs

```
FOR EACH brief_spec IN connection_briefs_to_generate:
    connection_id = brief_spec.connection_id
    action = brief_spec.action

    LOG INFO: "Generating connection brief: {connection_id} ({action})"

    # Get connection data
    connection_data = connection_contexts.connections[connection_id]
    co_occurrence_data = co_occurrence.pairs[connection_id]

    # Get contexts to analyze
    IF action == "CREATE":
        contexts_to_analyze = connection_data.contexts  # All contexts
    ELSE:
        contexts_to_analyze = brief_spec.new_contexts  # Only new contexts

    # Generate brief
    IF action == "CREATE":
        brief_content = GENERATE_NEW_CONNECTION_BRIEF(
            connection_id=connection_id,
            connection_data=connection_data,
            co_occurrence_data=co_occurrence_data,
            contexts=contexts_to_analyze,
            template_path="\\192.168.1.139\continuum\templates\connection-brief-template.md"
        )

    ELSE:  # UPDATE
        brief_content = UPDATE_CONNECTION_BRIEF(
            connection_id=connection_id,
            connection_data=connection_data,
            co_occurrence_data=co_occurrence_data,
            existing_brief=brief_spec.existing_brief,
            existing_metadata=brief_spec.existing_metadata,
            new_contexts=contexts_to_analyze
        )

    # Write brief
    brief_filename = NORMALIZE_FILENAME(f"{connection_data.entity1}_{connection_data.entity2}.md")
    brief_output_path = \\192.168.1.139\continuum\briefs\connections\{brief_filename}

    WRITE brief_content to brief_output_path

    LOG INFO: "Connection brief saved: {brief_output_path}"

    # Store for legal review
    connection_briefs_generated.append({
        "connection_id": connection_id,
        "brief_path": brief_output_path,
        "brief_content": brief_content,
        "action": action
    })
```

**Generate New Connection Brief Function:**
```
FUNCTION GENERATE_NEW_CONNECTION_BRIEF(connection_id, connection_data, co_occurrence_data, contexts, template_path):

    READ template from template_path

    # Use Claude to generate connection brief
    prompt = f'''
Generate an analytical intelligence brief for the connection between:
"{connection_data.entity1}" and "{connection_data.entity2}"

Connection Data:
- Sources Count: {co_occurrence_data.co_mention_count}
- First Documented: {co_occurrence_data.first_co_mention}
- Last Documented: {co_occurrence_data.last_co_mention}

Context Evidence:
{FORMAT_CONTEXTS(contexts)}

Template Structure:
{template}

Instructions:
1. Write executive summary describing the nature of the connection
2. Analyze relationship type (employment, representation, transaction, etc.)
3. Present evidence chronologically with source attributions
4. Create timeline of interactions
5. Synthesize relationship analysis from all contexts
6. Note any other entities that appear in connection contexts
7. Use neutral tone - present evidence without speculation
8. Flag any contradictions or ambiguities
9. Do NOT claim connections not supported by evidence

Return complete markdown brief following template structure.
'''

    brief_content = CALL_CLAUDE(prompt)

    # Add metadata
    metadata = {
        "connection_id": connection_id,
        "entity1": connection_data.entity1,
        "entity2": connection_data.entity2,
        "brief_version": "1.0",
        "last_updated": current_timestamp,
        "sources_count": co_occurrence_data.co_mention_count,
        "context_sources": [c.source_id for c in contexts],
        "legal_review": "PENDING"
    }

    final_brief = FORMAT_BRIEF_WITH_METADATA(metadata, brief_content)

    RETURN final_brief
```

### Step 6: Run Legal Compliance Review

**Action:** Execute legal-auditor agent on each generated brief

```
legal_review_results = []

# Combine all generated briefs
all_briefs = entity_briefs_generated + connection_briefs_generated

FOR EACH brief IN all_briefs:
    LOG INFO: "Running legal review on {brief.brief_path}"

    # Call legal-auditor agent
    legal_result = RUN_LEGAL_AUDITOR(brief.brief_content)

    # Process results
    IF legal_result.all_checks_pass:
        LOG INFO: "Legal review PASSED for {brief.brief_path}"

        # Update brief with auto-approval
        updated_brief = ADD_LEGAL_APPROVAL(
            brief_content=brief.brief_content,
            review_result=legal_result
        )

        # Rewrite brief with legal approval
        WRITE updated_brief to brief.brief_path

        legal_review_results.append({
            "brief_path": brief.brief_path,
            "status": "AUTO-APPROVED",
            "issues": []
        })

    ELSE:
        LOG WARNING: "Legal review FAILED for {brief.brief_path}"
        LOG WARNING: "Issues: {legal_result.issues}"

        # Update brief with issues found
        updated_brief = ADD_LEGAL_ISSUES(
            brief_content=brief.brief_content,
            review_result=legal_result
        )

        # Rewrite brief with legal issues
        WRITE updated_brief to brief.brief_path

        legal_review_results.append({
            "brief_path": brief.brief_path,
            "status": "ISSUES FOUND",
            "issues": legal_result.issues
        })
```

**Legal Auditor Function:**
```
FUNCTION RUN_LEGAL_AUDITOR(brief_content):

    # 18-point legal compliance checklist
    checklist_prompt = f'''
Review this intelligence brief for legal compliance using the 18-point checklist.

Brief Content:
---
{brief_content}
---

CHECKLIST (all must pass):

1. SOURCE ATTRIBUTION: Every factual claim attributed to specific source
2. NO SPECULATION: No unsupported inferences or assumptions
3. NO DEFAMATION: No false statements that could harm reputation
4. NEUTRAL TONE: Factual presentation without bias or prejudice
5. NO PRIVATE FACTS: No publication of private information without public interest
6. NO INTRUSION: Information obtained from public sources only
7. CONTEXT PROVIDED: Facts presented with appropriate context
8. NO MISREPRESENTATION: Quotes and statements not taken out of context
9. PUBLIC INTEREST: Information serves legitimate public interest
10. NO FINANCIAL HARM: No false statements about business/financial status
11. VERIFIABLE CLAIMS: All claims can be verified from cited sources
12. NO EMOTIONAL DISTRESS: No intentional infliction of distress
13. PROPER DISCLAIMERS: Appropriate caveats for unverified information
14. NO CRIMINAL ALLEGATIONS: No unsupported allegations of criminal activity
15. PRIVACY BALANCE: Private information balanced against public interest
16. NO CONFIDENTIAL INFO: No breach of confidential information
17. FAIR REPRESENTATION: All parties represented fairly
18. CORRECTIONS NOTED: Any contradictions or inconsistencies noted

For EACH point, return:
- pass: true/false
- reason: Brief explanation
- problematic_text: Quote from brief if failed (null if passed)

Return JSON:
{
  "checklist_results": [
    {
      "point": 1,
      "description": "SOURCE ATTRIBUTION",
      "pass": true,
      "reason": "All claims properly attributed",
      "problematic_text": null
    },
    ...
  ],
  "overall_pass": true,
  "summary": "Brief meets all legal compliance requirements"
}
'''

    result = CALL_CLAUDE(prompt)

    RETURN {
        "all_checks_pass": result.overall_pass,
        "issues": [r for r in result.checklist_results if not r.pass],
        "full_results": result
    }
```

**Add Legal Approval Function:**
```
FUNCTION ADD_LEGAL_APPROVAL(brief_content, review_result):

    # Parse existing frontmatter
    metadata = PARSE_YAML_FRONTMATTER(brief_content)
    content_body = EXTRACT_CONTENT_BODY(brief_content)

    # Update metadata
    metadata["legal_review"] = "AUTO-APPROVED"
    metadata["legal_review_date"] = current_timestamp
    metadata["legal_reviewer"] = "legal-auditor-agent"
    metadata["legal_checklist_pass"] = "18/18"

    # Rebuild brief
    updated_brief = FORMAT_BRIEF_WITH_METADATA(metadata, content_body)

    RETURN updated_brief
```

**Add Legal Issues Function:**
```
FUNCTION ADD_LEGAL_ISSUES(brief_content, review_result):

    # Parse existing frontmatter
    metadata = PARSE_YAML_FRONTMATTER(brief_content)
    content_body = EXTRACT_CONTENT_BODY(brief_content)

    # Update metadata
    metadata["legal_review"] = "ISSUES FOUND"
    metadata["legal_review_date"] = current_timestamp
    metadata["legal_reviewer"] = "legal-auditor-agent"
    metadata["legal_issues"] = [issue.description for issue in review_result.issues]
    metadata["legal_checklist_pass"] = f"{18 - len(review_result.issues)}/18"

    # Rebuild brief
    updated_brief = FORMAT_BRIEF_WITH_METADATA(metadata, content_body)

    RETURN updated_brief
```

### Step 7: Move Briefs to Pending Approval

**Action:** Copy all generated/updated briefs to pending_approval/ directory

```
FOR EACH brief IN all_briefs:
    # Determine destination
    IF brief in entity_briefs_generated:
        dest_dir = "\\192.168.1.139\continuum\pending_approval\entities\"
    ELSE:
        dest_dir = "\\192.168.1.139\continuum\pending_approval\connections\"

    # Get filename
    filename = BASENAME(brief.brief_path)

    # Copy to pending approval
    dest_path = dest_dir + filename
    COPY file from brief.brief_path to dest_path

    LOG INFO: "Moved to pending approval: {dest_path}"
```

### Step 8: Generate Review Log

**Action:** Create summary of what changed for human reviewer

```
review_log_content = f'''
# Review Log - {current_date}

Generated: {current_timestamp}
Stage 3 Processing Summary

## Entity Briefs

### Created ({count_entity_creates})
{LIST entity briefs created}

### Updated ({count_entity_updates})
{LIST entity briefs updated with reasons}

## Connection Briefs

### Created ({count_connection_creates})
{LIST connection briefs created}

### Updated ({count_connection_updates})
{LIST connection briefs updated with reasons}

## Legal Review Summary

**Auto-Approved:** {count_auto_approved}
**Issues Found:** {count_issues_found}

### Briefs with Legal Issues
{LIST briefs with legal issues and specific issues}

## Review Instructions

1. Review each brief in pending_approval/ directories
2. For briefs with legal issues, address flagged problems
3. For all briefs, verify accuracy and editorial quality
4. When satisfied, move approved briefs to approved/ directory
5. For briefs requiring changes, move back to briefs/ with notes

---
*Auto-generated by Stage 3: Brief Generation*
'''

WRITE review_log_content to \\192.168.1.139\continuum\pending_approval\REVIEW_LOG.md

LOG INFO: "Review log created"
```

### Step 9: Update Processing Timestamp

**Action:** Record completion timestamp for next run

```
WRITE current_timestamp to \\192.168.1.139\continuum\logs\stage3_last_run.txt

LOG INFO: "Stage 3 complete. {len(all_briefs)} briefs generated and awaiting approval."
```

## 6. Decision Logic

### 6.1 Significant New Information Threshold

**Decision:** When does new source material warrant updating an existing brief?

```
Criteria for SIGNIFICANT new information:
- New direct quotes or statements
- New connections with other entities
- New timeline events or activities
- Changed role, status, or position
- Contradictory information
- Legal developments or proceedings
- New business transactions
- Geographic movements

NOT significant:
- Same information from different source
- Minor variations of existing quotes
- Background information already covered
- Tangential mentions
- Redundant associations

DECISION RULE:
IF new_source contains >= 2 of significant criteria:
    UPDATE brief
ELSE:
    LOG "No significant update" and SKIP
```

### 6.2 Connection Brief Creation Threshold

**Decision:** When do co-occurrences warrant a connection brief?

**ARCHITECTURAL PRINCIPLE:** Connection briefs are the source of truth. If a connection exists in source documents with a quote, it warrants a brief.

```
Criteria:
1. Co-mention count >= 2 (minimum evidence threshold)
2. At least 1 context with extractable quote
3. Source document can be hosted (linked)

DECISION TREE:

IF co_mention_count < 2:
    → SKIP (insufficient evidence)

ELSE IF no contexts with extractable quotes:
    → SKIP (no substantive relationship to document)

ELSE:
    → CREATE brief with quote + source + summary
```

### 6.3 Legal Review Failure Handling

**Decision:** What to do when legal review finds issues?

```
IF legal_review.all_checks_pass == False:

    # Still proceed to pending_approval (human must review)
    brief.metadata.legal_review = "ISSUES FOUND"
    brief.metadata.legal_issues = list of issues

    # Add prominent warning to brief
    PREPEND to brief body:
    '''
    ## LEGAL REVIEW ALERT

    **STATUS:** Issues Found
    **REVIEWER:** legal-auditor-agent
    **DATE:** {timestamp}

    **Issues Requiring Attention:**
    {formatted list of issues with problematic text}

    Human review REQUIRED before publication.
    '''

    PROCEED to pending_approval/

    LOG WARNING: "Brief {brief_path} has legal issues, requires human review"

# Do NOT block brief from pending_approval
# Human has final authority on legal decisions
```

### 6.4 Brief Version Incrementing

**Decision:** How to version updated briefs?

```
Version format: MAJOR.MINOR

MAJOR increment (1.0 → 2.0):
- Significant new information that changes overall assessment
- Major corrections or contradictions
- Structural changes to brief

MINOR increment (1.0 → 1.1):
- Additional sources added
- New connections added
- Timeline updates
- Minor clarifications

DECISION LOGIC:

IF update_type == "major_revision":
    new_version = INCREMENT_MAJOR(current_version)
ELSE:
    new_version = INCREMENT_MINOR(current_version)

EXAMPLE:
Current: 2.3
Minor update: 2.4
Major update: 3.0
```

## 7. Outputs

### Primary Outputs

**Entity Briefs (New/Updated):**
`\\192.168.1.139\continuum\briefs\entity\analytical_brief_{entity}.md`
- Comprehensive analytical briefs for entities
- Updated with new source material
- Legal review status in metadata

**Connection Briefs (New/Updated):**
`\\192.168.1.139\continuum\briefs\connections\{entity1}_{entity2}.md`
- Relationship analysis briefs for entity pairs
- Context evidence and timeline
- Legal review status in metadata

**Pending Approval Copies:**
`\\192.168.1.139\continuum\pending_approval\entities\`
`\\192.168.1.139\continuum\pending_approval\connections\`
- Copies of all new/updated briefs awaiting human review

**Review Log:**
`\\192.168.1.139\continuum\pending_approval\REVIEW_LOG.md`
- Summary of what was generated/updated
- Legal review results
- Instructions for human reviewer

### Secondary Outputs

**Processing Timestamp:**
`\\192.168.1.139\continuum\logs\stage3_last_run.txt`
- Timestamp of last Stage 3 completion

### Log Outputs

**Processing Log:**
`\\192.168.1.139\continuum\logs\stage3_brief_generation.log`

Example:
```
2025-12-25T10:17:00Z | INFO | Stage 3 triggered by connection_contexts.json change
2025-12-25T10:17:01Z | INFO | Identified 15 affected entities
2025-12-25T10:17:01Z | INFO | Identified 7 changed connections
2025-12-25T10:17:02Z | INFO | Existing brief found for John Doe
2025-12-25T10:17:05Z | INFO | Entity John Doe has 2 new sources
2025-12-25T10:17:10Z | INFO | Queued UPDATE for John Doe
2025-12-25T10:17:30Z | INFO | Generating entity brief: John Doe (UPDATE)
2025-12-25T10:18:15Z | INFO | Entity brief saved
2025-12-25T10:18:20Z | INFO | Running legal review
2025-12-25T10:18:35Z | INFO | Legal review PASSED
2025-12-25T10:18:36Z | INFO | Moved to pending approval
2025-12-25T10:19:00Z | INFO | Review log created
2025-12-25T10:19:00Z | INFO | Stage 3 complete. 12 briefs generated and awaiting approval.
```

## 8. Success Criteria

### Mandatory Criteria

1. **Briefs Generated:** All affected entities/connections have briefs created or updated
2. **Legal Review Completed:** All briefs processed through legal-auditor
3. **Pending Approval Populated:** All briefs copied to pending_approval/
4. **Review Log Created:** REVIEW_LOG.md exists and summarizes changes
5. **No Data Corruption:** All brief files are valid markdown with YAML frontmatter

### Quality Criteria

1. **Content Quality:** Briefs are coherent, well-structured, and comprehensive
2. **Legal Pass Rate:** > 80% of briefs auto-approved by legal review
3. **Source Coverage:** Briefs reference all relevant sources
4. **No Hallucinations:** All facts traceable to source material
5. **Consistent Formatting:** All briefs follow template structure

### Validation Commands

```bash
# Count briefs generated
ls \\192.168.1.139\continuum\pending_approval\entities\ | wc -l
ls \\192.168.1.139\continuum\pending_approval\connections\ | wc -l

# Check legal approval rate
grep -r "legal_review: \"AUTO-APPROVED\"" \\192.168.1.139\continuum\pending_approval\ | wc -l

# Validate YAML frontmatter
for file in \\192.168.1.139\continuum\pending_approval\entities\*.md; do
    python -c "import yaml; yaml.safe_load(open('$file').read().split('---')[1])"
done

# Check review log exists
ls -lh \\192.168.1.139\continuum\pending_approval\REVIEW_LOG.md
```

## 9. Error Handling

### 9.1 Claude API Failure During Brief Generation

**Error:** Claude API unavailable or times out

```
CATCH APIError during GENERATE_NEW_ENTITY_BRIEF OR UPDATE_ENTITY_BRIEF:
    LOG ERROR: "Brief generation failed for {entity_name}: {error}"

    # Retry with exponential backoff
    FOR attempt in [1, 2, 3]:
        WAIT (2 ** attempt) seconds
        TRY:
            brief_content = GENERATE_BRIEF(...)
            BREAK  # Success
        CATCH APIError:
            IF attempt == 3:
                # All retries failed
                LOG CRITICAL: "Brief generation failed after 3 retries: {entity_name}"

                # Create placeholder brief for human completion
                placeholder = CREATE_PLACEHOLDER_BRIEF(entity_name, source_data)
                WRITE placeholder to brief_path

                # Flag for manual completion
                ADD to review_queue: {
                    "issue": "generation_failed",
                    "entity": entity_name,
                    "error": error,
                    "action_required": "Complete brief manually"
                }

                CONTINUE to next brief
```

**Placeholder Brief Template:**
```markdown
---
entity_name: "{ENTITY_NAME}"
brief_version: "0.1"
status: "GENERATION_FAILED"
error: "{ERROR_MESSAGE}"
requires_manual_completion: true
---

# Analytical Brief: {ENTITY_NAME}

**GENERATION FAILED - MANUAL COMPLETION REQUIRED**

## Available Data

**Entity Type:** {ENTITY_TYPE}
**Sources:** {SOURCE_LIST}

## Source Material
{RAW_SOURCE_CONTEXTS}

## Instructions for Manual Completion
Complete this brief manually using the source material above.
Follow template: \\192.168.1.139\continuum\templates\entity-brief-template.md
```

### 9.2 Legal Review Agent Failure

**Error:** Legal-auditor agent unavailable

```
CATCH AgentError during RUN_LEGAL_AUDITOR:
    LOG ERROR: "Legal review failed for {brief_path}: {error}"

    # Add legal review failure notice to brief
    metadata.legal_review = "REVIEW_FAILED"
    metadata.legal_review_error = error
    metadata.requires_manual_legal_review = true

    # Still proceed to pending_approval
    # Human MUST perform legal review manually

    LOG WARNING: "Brief {brief_path} requires manual legal review"

    PROCEED to pending_approval/
```

### 9.3 File Write Permission Error

**Error:** Cannot write to briefs/ or pending_approval/ directory

```
CATCH PermissionError during WRITE:
    LOG CRITICAL: "Cannot write brief to {path}: Permission denied"

    # Try alternate location
    emergency_path = "\\192.168.1.139\continuum\backups\emergency\briefs\{filename}"
    WRITE brief to emergency_path

    LOG INFO: "Brief saved to emergency backup: {emergency_path}"

    # Alert operator
    SEND alert: "Stage 3 cannot write to primary brief locations"

    # Continue with other briefs
    CONTINUE
```

### 9.4 Template File Missing

**Error:** Brief template file not found

```
CATCH FileNotFoundError when reading template:
    LOG ERROR: "Template file not found: {template_path}"

    # Use embedded fallback template
    fallback_template = GET_EMBEDDED_FALLBACK_TEMPLATE(template_type)

    LOG WARNING: "Using embedded fallback template"

    PROCEED with fallback_template
```

## 10. Next Stage

**Stage 4: Publication (SOP-004)**

**Trigger Condition:** Files detected in `\\192.168.1.139\continuum\approved\` directories

**What Happens Before Stage 4:**
**HUMAN INTERVENTION REQUIRED:**
1. Human reviews briefs in `pending_approval/` directories
2. Human reads `REVIEW_LOG.md` for context on changes
3. Human moves approved briefs to `approved/` directories
4. Human addresses any legal issues before approving

**What Stage 4 Will Do:**
- Parse approved brief metadata
- Update website JSON data files
- Copy briefs to website/briefs/
- Copy source PDFs to website/sources/
- Archive published briefs
- Clean up approved/ directory

**Handoff Requirements:**
- Briefs in approved/ must have valid YAML frontmatter
- All referenced sources must be accessible
- Legal review status must be resolved (either AUTO-APPROVED or human-reviewed)

**Timing:** Stage 4 can run immediately when files appear in approved/, or on scheduled basis (e.g., daily publication run)

---

## Appendix A: YAML Frontmatter Schema

### Entity Brief Metadata
```yaml
---
entity_name: "John Doe"
entity_type: "PERSON"
brief_version: "1.2"
last_updated: "2025-12-25T10:18:00Z"
sources_count: 12
sources_covered: ["src_000042", "src_000103", ...]
legal_review: "AUTO-APPROVED"
legal_review_date: "2025-12-25T10:18:35Z"
legal_reviewer: "legal-auditor-agent"
legal_checklist_pass: "18/18"
---
```

### Connection Brief Metadata
```yaml
---
connection_id: "John Doe|Acme Corporation"
entity1: "Acme Corporation"
entity2: "John Doe"
brief_version: "1.0"
last_updated: "2025-12-25T10:19:00Z"
sources_count: 6
context_sources: ["src_000042", "src_000245"]
legal_review: "AUTO-APPROVED"
legal_review_date: "2025-12-25T10:19:15Z"
legal_reviewer: "legal-auditor-agent"
legal_checklist_pass: "18/18"
---
```

## Appendix B: Filename Normalization

```python
def NORMALIZE_FILENAME(name):
    # Replace invalid filename characters
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        name = name.replace(char, '_')

    # Replace spaces with underscores
    name = name.replace(' ', '_')

    # Remove multiple consecutive underscores
    while '__' in name:
        name = name.replace('__', '_')

    # Limit length (Windows MAX_PATH consideration)
    if len(name) > 200:
        name = name[:200]

    # Ensure .md extension
    if not name.endswith('.md'):
        name += '.md'

    return name
```

## Appendix C: Legal Compliance Checklist Reference

See function `RUN_LEGAL_AUDITOR` in Step 6 for complete 18-point checklist.

Key areas:
- Source attribution (points 1, 11)
- Defamation protection (points 3, 10, 14)
- Privacy considerations (points 5, 6, 15, 16)
- Factual accuracy (points 2, 8, 9, 13)
- Fair representation (points 4, 7, 17, 18)

---

**Document Control**
- **Related SOPs:** SOP-002 (Context Extraction), SOP-004 (Publication)
- **Next Review Date:** 2026-03-25
- **Change Authority:** Pipeline Administrator
