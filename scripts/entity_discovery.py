#!/usr/bin/env python3
"""
Entity Discovery Pipeline for The Continuum Report
Version 2.0 - Refactored with Shared Library

This script:
1. Fetches ALL documents from Paperless-ngx
2. Extracts and saves document content for batch processing
3. Maintains an entity database (JSON) tracking all discovered entities
4. Supports checkpointing for overnight batch runs

Changes in v2.0:
- Uses centralized configuration (lib.config)
- Uses PaperlessClient with retry logic (lib.paperless_client)
- Structured logging with structlog (lib.logging_config)
- No hardcoded secrets - uses environment variables
"""

import json
import os
import sys
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set

# Import shared library
from lib import settings, get_logger, PaperlessClient, PaperlessError

# Initialize logger
logger = get_logger(__name__)

# =============================================================================
# PATHS (from centralized config)
# =============================================================================

DATA_DIR = settings.data_dir
CHECKPOINT_FILE = DATA_DIR / "discovery_checkpoint.json"
ENTITY_DB_FILE = settings.entity_db_file
DOSSIER_QUEUE_FILE = settings.dossier_queue_file
REPORTS_DIR = settings.reports_dir

# Ensure directories exist
settings.ensure_directories()


# =============================================================================
# PAPERLESS API (using PaperlessClient)
# =============================================================================

def get_paperless_client() -> PaperlessClient:
    """Get a configured Paperless client."""
    return PaperlessClient()


def fetch_all_documents(exclude_dossiers: bool = True) -> List[Dict]:
    """Fetch all documents from Paperless with pagination."""
    logger.info("Fetching documents from Paperless-ngx", exclude_dossiers=exclude_dossiers)

    try:
        with get_paperless_client() as client:
            def progress(fetched, total):
                logger.debug("Fetch progress", fetched=fetched, total=total)

            all_docs = client.get_all_documents(
                exclude_dossiers=exclude_dossiers,
                progress_callback=progress
            )

            logger.info("Documents fetched", count=len(all_docs))
            return all_docs

    except PaperlessError as e:
        logger.error("Failed to fetch documents", error=str(e))
        return []


def get_document_content(doc_id: int) -> Optional[str]:
    """Get full document content."""
    try:
        with get_paperless_client() as client:
            return client.get_document_content(doc_id)
    except PaperlessError as e:
        logger.error("Failed to get document content", doc_id=doc_id, error=str(e))
        return None


def get_document_details(doc_id: int) -> Optional[Dict]:
    """Get full document details."""
    try:
        with get_paperless_client() as client:
            return client.get_document(doc_id)
    except PaperlessError as e:
        logger.error("Failed to get document details", doc_id=doc_id, error=str(e))
        return None


def verify_primary_source(content: str, title: str = "") -> Dict:
    """
    Analyze document content to verify it's a primary source, not AI-generated.

    Returns:
        {
            "is_primary": bool,
            "source_type": str,  # "court_filing", "deposition", "book", "unknown", "ai_generated"
            "confidence": str,   # "high", "medium", "low"
            "reason": str
        }
    """
    content_lower = content.lower()
    first_2000 = content[:2000].lower()

    # AI-generated indicators (check first)
    ai_indicators = [
        "this document provides",
        "document overview",
        "this dossier",
        "executive summary\n",
        "key findings:",
        "generated by",
        "ai-assisted",
        "this analysis",
        "this report synthesizes",
        "the following analysis"
    ]

    ai_score = sum(1 for ind in ai_indicators if ind in first_2000)
    if ai_score >= 2:
        return {
            "is_primary": False,
            "source_type": "ai_generated",
            "confidence": "high",
            "reason": f"Multiple AI-generated indicators found ({ai_score})"
        }

    # Court filing indicators
    court_indicators = [
        "case 1:",
        "case no.",
        "united states district court",
        "filed",
        "document",
        "plaintiff",
        "defendant",
        "v.",
        "docket"
    ]
    court_score = sum(1 for ind in court_indicators if ind in first_2000)
    if court_score >= 4:
        return {
            "is_primary": True,
            "source_type": "court_filing",
            "confidence": "high",
            "reason": f"Court filing format detected ({court_score} indicators)"
        }

    # Deposition indicators
    depo_indicators = [
        "q.",
        "a.",
        "the witness:",
        "by mr.",
        "by ms.",
        "deposition",
        "sworn",
        "testimony"
    ]
    depo_score = sum(1 for ind in depo_indicators if ind in first_2000)
    if depo_score >= 4:
        return {
            "is_primary": True,
            "source_type": "deposition",
            "confidence": "high",
            "reason": f"Deposition transcript format detected ({depo_score} indicators)"
        }

    # Book indicators - look for copyright, publisher, ISBN
    book_indicators = ["copyright Â©", "published by", "isbn", "all rights reserved"]
    book_score = sum(1 for ind in book_indicators if ind in first_2000)
    if book_score >= 2:
        return {
            "is_primary": True,
            "source_type": "book",
            "confidence": "high",
            "reason": "Book format detected (copyright/publisher info found)"
        }

    # Book/chapter indicators - secondary check
    if "chapter" in first_2000 or "page" in first_2000:
        # Could be a book - check for narrative prose vs summary
        if ai_score == 0:
            return {
                "is_primary": True,
                "source_type": "book",
                "confidence": "medium",
                "reason": "Book format detected, no AI indicators"
            }

    # Email/correspondence
    if "from:" in first_2000 and "to:" in first_2000:
        return {
            "is_primary": True,
            "source_type": "correspondence",
            "confidence": "high",
            "reason": "Email/correspondence format detected"
        }

    # If we have some court indicators but not enough for high confidence
    if court_score >= 2:
        return {
            "is_primary": True,
            "source_type": "court_filing",
            "confidence": "medium",
            "reason": f"Likely court document ({court_score} indicators)"
        }

    # Default - unknown, needs manual review
    return {
        "is_primary": None,  # Unknown
        "source_type": "unknown",
        "confidence": "low",
        "reason": "Could not determine document type - needs manual review"
    }


# =============================================================================
# ENTITY DATABASE
# =============================================================================

def load_entity_db() -> Dict:
    """Load or initialize entity database."""
    if ENTITY_DB_FILE.exists():
        try:
            with open(ENTITY_DB_FILE, 'r') as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            logger.warning("Failed to load entity database", error=str(e))

    return {
        "people": {},       # name -> {docs: [ids], contexts: [str], roles: [str]}
        "organizations": {},
        "events": {},       # event_name -> {docs: [ids], dates: [str], descriptions: [str]}
        "locations": {},
        "processed_docs": [],  # List of doc IDs already processed
        "last_updated": None
    }


def save_entity_db(db: Dict):
    """Save entity database."""
    db["last_updated"] = datetime.now().isoformat()
    with open(ENTITY_DB_FILE, 'w') as f:
        json.dump(db, f, indent=2)
    logger.debug("Entity database saved", path=str(ENTITY_DB_FILE))


def add_entity(db: Dict, entity_type: str, name: str, doc_id: int,
               context: str = "", role: str = "", date: str = ""):
    """Add or update an entity in the database."""
    # Normalize name
    name = name.strip()
    if not name or len(name) < 2:
        return

    # Get or create entity entry
    if name not in db[entity_type]:
        db[entity_type][name] = {
            "docs": [],
            "contexts": [],
            "first_seen": datetime.now().isoformat()
        }
        if entity_type == "people":
            db[entity_type][name]["roles"] = []
        if entity_type == "events":
            db[entity_type][name]["dates"] = []

    entry = db[entity_type][name]

    # Add document reference
    if doc_id not in entry["docs"]:
        entry["docs"].append(doc_id)

    # Add context if new
    if context and context not in entry["contexts"]:
        entry["contexts"].append(context[:500])  # Limit context length

    # Add role for people
    if entity_type == "people" and role and role not in entry.get("roles", []):
        entry["roles"].append(role)

    # Add date for events
    if entity_type == "events" and date and date not in entry.get("dates", []):
        entry["dates"].append(date)


# =============================================================================
# CHECKPOINT SYSTEM
# =============================================================================

def load_checkpoint() -> Dict:
    """Load checkpoint data."""
    if CHECKPOINT_FILE.exists():
        try:
            with open(CHECKPOINT_FILE, 'r') as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            logger.warning("Failed to load checkpoint", error=str(e))
    return {"processed_doc_ids": [], "current_batch": 0, "started": None}


def save_checkpoint(data: Dict):
    """Save checkpoint data."""
    data["last_updated"] = datetime.now().isoformat()
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(data, f, indent=2)
    logger.debug("Checkpoint saved", path=str(CHECKPOINT_FILE))


# =============================================================================
# DOSSIER QUEUE
# =============================================================================

def generate_dossier_queue(db: Dict, min_docs: int = 2) -> List[Dict]:
    """Generate prioritized queue of entities needing dossiers."""
    queue = []

    for entity_type in ["people", "organizations", "events"]:
        for name, data in db.get(entity_type, {}).items():
            doc_count = len(data.get("docs", []))
            if doc_count >= min_docs:
                queue.append({
                    "type": entity_type,
                    "name": name,
                    "doc_count": doc_count,
                    "docs": data["docs"],
                    "contexts": data.get("contexts", [])[:5],
                    "roles": data.get("roles", []) if entity_type == "people" else [],
                    "priority_score": doc_count * (3 if entity_type == "people" else 2 if entity_type == "organizations" else 1)
                })

    # Sort by priority (most documents first, people weighted higher)
    queue.sort(key=lambda x: x["priority_score"], reverse=True)

    return queue


def save_dossier_queue(queue: List[Dict]):
    """Save dossier queue."""
    with open(DOSSIER_QUEUE_FILE, 'w') as f:
        json.dump({
            "generated": datetime.now().isoformat(),
            "total_entities": len(queue),
            "queue": queue
        }, f, indent=2)
    logger.info("Dossier queue saved", count=len(queue), path=str(DOSSIER_QUEUE_FILE))


# =============================================================================
# OUTPUT FUNCTIONS
# =============================================================================

def export_document_batch(docs: List[Dict], batch_num: int, batch_size: int = 10):
    """Export a batch of documents for Claude processing."""
    batch_file = DATA_DIR / f"doc_batch_{batch_num:03d}.json"

    batch_data = []
    for doc in docs:
        doc_id = doc["id"]
        content = get_document_content(doc_id)

        if content:
            # Truncate very long documents
            if len(content) > 15000:
                content = content[:7500] + "\n\n[...CONTENT TRUNCATED...]\n\n" + content[-7500:]

            batch_data.append({
                "id": doc_id,
                "title": doc.get("title", f"Document {doc_id}"),
                "created": doc.get("created", ""),
                "document_type": doc.get("document_type"),
                "content": content
            })

    with open(batch_file, 'w') as f:
        json.dump(batch_data, f, indent=2)

    logger.info("Batch exported", batch=batch_num, documents=len(batch_data), path=str(batch_file))
    return batch_file


def print_entity_summary(db: Dict):
    """Print summary of discovered entities."""
    print("\n" + "="*60)
    print("ENTITY DISCOVERY SUMMARY")
    print("="*60)

    for entity_type in ["people", "organizations", "events", "locations"]:
        entities = db.get(entity_type, {})
        multi_doc = {k: v for k, v in entities.items() if len(v.get("docs", [])) >= 2}

        print(f"\n{entity_type.upper()}: {len(entities)} total, {len(multi_doc)} in 2+ docs")

        # Show top entities
        sorted_entities = sorted(entities.items(), key=lambda x: len(x[1].get("docs", [])), reverse=True)
        for name, data in sorted_entities[:10]:
            doc_count = len(data.get("docs", []))
            print(f"  - {name}: {doc_count} docs")


# =============================================================================
# MAIN COMMANDS
# =============================================================================

def cmd_status():
    """Show current status."""
    print("\n" + "="*60)
    print("ENTITY DISCOVERY STATUS")
    print("="*60)

    # Check Paperless
    try:
        with get_paperless_client() as client:
            if client.health_check():
                docs = client.get_documents_page(page_size=1)
                total_docs = docs.get("count", 0)
                print(f"\nPaperless: Connected ({total_docs} documents)")
            else:
                print("\nPaperless: Health check failed")
    except PaperlessError as e:
        print(f"\nPaperless: Connection failed - {e}")

    # Load checkpoint
    checkpoint = load_checkpoint()
    processed = len(checkpoint.get("processed_doc_ids", []))
    print(f"Checkpoint: {processed} documents processed")

    # Load entity database
    db = load_entity_db()
    print(f"\nEntity Database:")
    print(f"  People: {len(db.get('people', {}))}")
    print(f"  Organizations: {len(db.get('organizations', {}))}")
    print(f"  Events: {len(db.get('events', {}))}")
    print(f"  Locations: {len(db.get('locations', {}))}")

    # Show queue status
    if DOSSIER_QUEUE_FILE.exists():
        with open(DOSSIER_QUEUE_FILE, 'r') as f:
            queue_data = json.load(f)
        print(f"\nDossier Queue: {queue_data.get('total_entities', 0)} entities pending")


def cmd_fetch():
    """Fetch all documents and export for processing."""
    docs = fetch_all_documents(exclude_dossiers=True)
    print(f"\nFetched {len(docs)} source documents")

    # Export in batches
    batch_size = 10
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i+batch_size]
        export_document_batch(batch, i // batch_size + 1, batch_size)

    print(f"\nExported {len(docs)} documents in {(len(docs) + batch_size - 1) // batch_size} batches")
    print(f"Batch files saved to: {DATA_DIR}")


def cmd_list_docs():
    """List all documents with IDs."""
    docs = fetch_all_documents(exclude_dossiers=True)
    print(f"\n{'ID':<6} {'Title':<60} {'Type'}")
    print("-"*80)
    for doc in docs:
        title = doc.get("title", "Untitled")[:58]
        doc_type = doc.get("document_type", "-")
        print(f"{doc['id']:<6} {title:<60} {doc_type}")


def cmd_export_single(doc_id: int):
    """Export a single document for inspection."""
    doc = get_document_details(doc_id)
    if doc:
        print(f"\n{'='*60}")
        print(f"Document ID: {doc['id']}")
        print(f"Title: {doc.get('title', 'Untitled')}")
        print(f"Created: {doc.get('created', 'Unknown')}")
        print(f"Type: {doc.get('document_type', 'Unknown')}")
        print(f"Tags: {doc.get('tags', [])}")
        print(f"{'='*60}")
        print("\nCONTENT PREVIEW (first 3000 chars):\n")
        content = doc.get("content", "")
        print(content[:3000])
        if len(content) > 3000:
            print(f"\n[...{len(content) - 3000} more characters...]")
    else:
        print(f"Document {doc_id} not found")


def cmd_build_queue():
    """Build dossier queue from entity database."""
    db = load_entity_db()
    queue = generate_dossier_queue(db, min_docs=2)
    save_dossier_queue(queue)

    print(f"\nDossier Queue Generated: {len(queue)} entities")
    print("\nTop 20 priority entities:")
    print(f"{'Type':<15} {'Name':<40} {'Docs'}")
    print("-"*65)
    for item in queue[:20]:
        print(f"{item['type']:<15} {item['name'][:38]:<40} {item['doc_count']}")


def cmd_summary():
    """Show entity summary."""
    db = load_entity_db()
    print_entity_summary(db)


def cmd_add_entity(entity_type: str, name: str, doc_ids: List[int], context: str = "", role: str = ""):
    """Manually add an entity to the database."""
    db = load_entity_db()
    for doc_id in doc_ids:
        add_entity(db, entity_type, name, doc_id, context=context, role=role)
    save_entity_db(db)
    logger.info("Entity added", type=entity_type, name=name, docs=doc_ids)
    print(f"Added {entity_type}: {name} (docs: {doc_ids})")


def cmd_process_batch(batch_num: int):
    """Process a batch file and extract entities using patterns."""
    batch_file = DATA_DIR / f"doc_batch_{batch_num:03d}.json"

    if not batch_file.exists():
        print(f"Batch file not found: {batch_file}")
        return

    with open(batch_file, 'r') as f:
        docs = json.load(f)

    db = load_entity_db()
    checkpoint = load_checkpoint()
    processed = set(checkpoint.get("processed_doc_ids", []))

    # Known key figures to look for
    key_people = [
        "Jeffrey Epstein", "Ghislaine Maxwell", "Virginia Giuffre", "Virginia Roberts",
        "Prince Andrew", "Alan Dershowitz", "Bill Clinton", "Donald Trump",
        "Les Wexner", "Jean-Luc Brunel", "Sarah Kellen", "Nadia Marcinkova",
        "Lesley Groff", "Emmy Taylor", "Adriana Ross", "Kevin Spacey",
        "Bill Richardson", "George Mitchell", "Glenn Dubin", "Eva Andersson-Dubin",
        "Marvin Minsky", "Stephen Hawking", "Ehud Barak", "Larry Summers",
        "Bill Gates", "Tom Pritzker", "David Copperfield"
    ]

    key_orgs = [
        "Victoria's Secret", "L Brands", "Wexner Foundation", "The Limited",
        "The Terramar Project", "MC2 Model Management", "Epstein Foundation",
        "Clinton Foundation", "Clinton Global Initiative", "Harvard University",
        "MIT Media Lab", "Ohio State University", "Deutsche Bank", "JP Morgan",
        "Bear Stearns"
    ]

    key_locations = [
        "Little St. James", "Zorro Ranch", "9 East 71st", "71st Street",
        "Palm Beach", "Mar-a-Lago", "Buckingham Palace", "Manhattan",
        "New Mexico", "Virgin Islands", "Paris", "London"
    ]

    logger.info("Processing batch", batch=batch_num, documents=len(docs))
    print(f"\nProcessing batch {batch_num}: {len(docs)} documents")
    print("-" * 60)

    for doc in docs:
        doc_id = doc["id"]
        title = doc.get("title", "")
        content = doc.get("content", "").lower()

        if doc_id in processed:
            logger.debug("Document already processed", doc_id=doc_id)
            print(f"  [{doc_id}] Already processed, skipping")
            continue

        print(f"\n  [{doc_id}] {title[:50]}")

        # Extract people
        people_found = []
        for person in key_people:
            if person.lower() in content:
                people_found.append(person)
                add_entity(db, "people", person, doc_id,
                          context=f"Found in {title}")

        # Extract organizations
        orgs_found = []
        for org in key_orgs:
            if org.lower() in content:
                orgs_found.append(org)
                add_entity(db, "organizations", org, doc_id,
                          context=f"Found in {title}")

        # Extract locations
        locs_found = []
        for loc in key_locations:
            if loc.lower() in content:
                locs_found.append(loc)
                add_entity(db, "locations", loc, doc_id,
                          context=f"Found in {title}")

        # Extract case references as events
        case_patterns = [
            (r"15-cv-7433|15-cv-07433", "Giuffre v. Maxwell Civil Case"),
            (r"20-cr-330", "USA v. Maxwell Criminal Case"),
            (r"08-cv-80736", "Epstein Florida Case"),
        ]

        for pattern, event_name in case_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                add_entity(db, "events", event_name, doc_id,
                          context=f"Referenced in {title}")

        # Print findings
        if people_found:
            print(f"    People: {', '.join(people_found[:5])}")
        if orgs_found:
            print(f"    Orgs: {', '.join(orgs_found[:5])}")
        if locs_found:
            print(f"    Locations: {', '.join(locs_found[:3])}")

        processed.add(doc_id)

    # Save progress
    save_entity_db(db)
    checkpoint["processed_doc_ids"] = list(processed)
    save_checkpoint(checkpoint)

    logger.info("Batch processing complete", batch=batch_num, processed=len(docs))
    print(f"\n  Batch {batch_num} complete. Processed {len(docs)} documents.")
    print_entity_summary(db)


def cmd_process_all():
    """Process all batches."""
    batch_num = 1
    while True:
        batch_file = DATA_DIR / f"doc_batch_{batch_num:03d}.json"
        if not batch_file.exists():
            break
        cmd_process_batch(batch_num)
        batch_num += 1

    print(f"\n{'='*60}")
    print("ALL BATCHES PROCESSED")
    print(f"{'='*60}")

    # Build the queue
    cmd_build_queue()


def cmd_show_doc_entities(doc_id: int):
    """Show which entities appear in a specific document."""
    db = load_entity_db()

    print(f"\nEntities in document {doc_id}:")
    print("-" * 40)

    for entity_type in ["people", "organizations", "events", "locations"]:
        matches = []
        for name, data in db.get(entity_type, {}).items():
            if doc_id in data.get("docs", []):
                matches.append(name)

        if matches:
            print(f"\n{entity_type.upper()}:")
            for name in matches:
                print(f"  - {name}")


def cmd_verify_sources():
    """Verify all documents are primary sources, flag AI-generated content."""
    docs = fetch_all_documents(exclude_dossiers=False)  # Get ALL docs including potential AI ones

    print(f"\nVerifying {len(docs)} documents...")
    print("=" * 70)

    ai_generated = []
    primary_sources = []
    needs_review = []

    for i, doc in enumerate(docs):
        doc_id = doc["id"]
        title = doc.get("title", "")

        content = get_document_content(doc_id)
        if not content:
            needs_review.append((doc_id, title, "Could not fetch content"))
            continue

        result = verify_primary_source(content, title)

        if result["is_primary"] == False:
            ai_generated.append((doc_id, title, result["reason"]))
        elif result["is_primary"] == True:
            primary_sources.append((doc_id, title, result["source_type"], result["confidence"]))
        else:
            needs_review.append((doc_id, title, result["reason"]))

        # Progress
        if (i + 1) % 25 == 0:
            print(f"  Checked {i + 1}/{len(docs)}...")

    print(f"\n{'=' * 70}")
    print("VERIFICATION RESULTS")
    print(f"{'=' * 70}")

    print(f"\n[OK] PRIMARY SOURCES: {len(primary_sources)}")
    high_conf = [p for p in primary_sources if p[3] == "high"]
    med_conf = [p for p in primary_sources if p[3] == "medium"]
    print(f"     High confidence: {len(high_conf)}")
    print(f"     Medium confidence: {len(med_conf)}")

    if ai_generated:
        print(f"\n[!!] AI-GENERATED (exclude these): {len(ai_generated)}")
        for doc_id, title, reason in ai_generated:
            print(f"     [{doc_id}] {title[:45]} - {reason}")

    if needs_review:
        print(f"\n[??] NEEDS MANUAL REVIEW: {len(needs_review)}")
        for doc_id, title, reason in needs_review:
            print(f"     [{doc_id}] {title[:45]} - {reason}")

    print(f"\n{'=' * 70}")


# =============================================================================
# CLI
# =============================================================================

def print_usage():
    print(f"""
Entity Discovery Pipeline - The Continuum Report
================================================

Commands:
  status              Show current status
  fetch               Fetch all documents and export batches
  list                List all documents with IDs
  doc <id>            Show single document content
  summary             Show entity database summary
  queue               Build/update dossier queue from entity DB
  process <batch>     Process a specific batch for entities
  process-all         Process ALL batches
  doc-entities <id>   Show entities found in a document
  verify              Verify all docs are primary sources (flag AI-generated)

Configuration:
  Paperless URL:      {settings.paperless_url}
  Base Directory:     {settings.continuum_base_dir}

Files:
  Entity Database:    {ENTITY_DB_FILE}
  Dossier Queue:      {DOSSIER_QUEUE_FILE}
  Checkpoint:         {CHECKPOINT_FILE}
  Document Batches:   {DATA_DIR}/doc_batch_*.json

Usage:
  python entity_discovery.py status
  python entity_discovery.py fetch
  python entity_discovery.py doc 123
  python entity_discovery.py process 1
  python entity_discovery.py process-all
""")


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print_usage()
        sys.exit(0)

    command = sys.argv[1].lower()

    try:
        if command == "status":
            cmd_status()
        elif command == "fetch":
            cmd_fetch()
        elif command == "list":
            cmd_list_docs()
        elif command == "doc" and len(sys.argv) > 2:
            cmd_export_single(int(sys.argv[2]))
        elif command == "summary":
            cmd_summary()
        elif command == "queue":
            cmd_build_queue()
        elif command == "process" and len(sys.argv) > 2:
            cmd_process_batch(int(sys.argv[2]))
        elif command == "process-all":
            cmd_process_all()
        elif command == "doc-entities" and len(sys.argv) > 2:
            cmd_show_doc_entities(int(sys.argv[2]))
        elif command == "verify":
            cmd_verify_sources()
        else:
            print_usage()
    except KeyboardInterrupt:
        logger.info("Operation cancelled by user")
        print("\nCancelled.")
    except PaperlessError as e:
        logger.error("Paperless error", error=str(e))
        print(f"\nError: {e}")
        print("Check your PAPERLESS_TOKEN in .env file")
        sys.exit(1)
